{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25abc122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "import random\n",
    "from model import EnhancedAudioCNN, train_model, eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fcde60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnhancedAudioCNN(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU()\n",
       "    (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU()\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU()\n",
       "    (17): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU()\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Dropout(p=0.3, inplace=False)\n",
       "    (5): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (6): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")  # or \"cuda\" if using GPU\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = EnhancedAudioCNN().to(device)\n",
    "model.load_state_dict(torch.load(\"audio_classification_model_augmented.pth\", map_location=device))\n",
    "model.eval()  # Set model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9217686c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running inferences on test spectrogram images:\n",
      "for cat_126.jpg, the prediction is cat.\n",
      "for cat_137.jpg, the prediction is cat.\n",
      "for cat_143.jpg, the prediction is cat.\n",
      "for cat_144.jpg, the prediction is dog.\n",
      "for cat_152.jpg, the prediction is cat.\n",
      "for cat_158.jpg, the prediction is cat.\n",
      "for cat_17.jpg, the prediction is cat.\n",
      "for cat_20.jpg, the prediction is cat.\n",
      "for cat_66.jpg, the prediction is cat.\n",
      "for cat_67.jpg, the prediction is cat.\n",
      "for cat_75.jpg, the prediction is dog.\n",
      "for cat_86.jpg, the prediction is cat.\n",
      "for cat_90.jpg, the prediction is cat.\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.Resize((224, 224)),transforms.ToTensor()])\n",
    "# Run inferences on test spectrogram images\n",
    "print(\"\\nRunning inferences on test spectrogram images:\")\n",
    "model.eval()\n",
    "inference_dir = r\"C:\\Users\\User\\Documents\\audio-recognition-master\\audio-recognition-master\\img_dataset\\inferences\"\n",
    "inference_dir = r\"C:\\Users\\User\\Documents\\audio-recognition-master\\audio-recognition-master\\img_dataset\\test\\cat\"\n",
    "\n",
    "for spectrogram in os.listdir(inference_dir):\n",
    "    if not spectrogram.endswith(\".jpg\"):\n",
    "        continue\n",
    "    try:\n",
    "        # Load the spectrogram image\n",
    "        img_path = os.path.join(inference_dir, spectrogram)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        # Apply transforms\n",
    "        img_tensor = transform(image)\n",
    "        # For CNN, we can use the image tensor directly\n",
    "        # Add batch dimension\n",
    "        features = img_tensor.unsqueeze(0).to(device)\n",
    "        # Make prediction\n",
    "        pred = model(features)\n",
    "        if pred[0, 0] < 0.5:\n",
    "            label = \"cat\"\n",
    "        else:\n",
    "            label = \"dog\"\n",
    "        print(f\"for {spectrogram}, the prediction is {label}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {spectrogram}: {e}\")\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraudvelmeni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
